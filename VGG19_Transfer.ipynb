{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    dirs = ['cat', 'lion', 'tiger', 'leopard']\n",
    "    lab = np.concatenate((np.tile(0, 50), np.tile(1,50), np.tile(2,50), np.tile(3,50)), axis = 0)\n",
    "    data_train = []\n",
    "    data_test = []\n",
    "\n",
    "    for dir_ in dirs:\n",
    "        files_train = glob.glob(os.path.join('data','train',dir_,'*.jpg'))\n",
    "        files_test = glob.glob(os.path.join('data','test',dir_,'*.jpg'))\n",
    "        for train, test in zip(files_train, files_test):\n",
    "            data_train.append(cv2.imread(train, 1))\n",
    "            data_test.append(cv2.imread(test, 1))\n",
    "\n",
    "    labels = to_categorical(lab)\n",
    "    return np.array(data_train), labels, np.array(data_test), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg19():\n",
    "    def __init__(self, vgg19_npy_path=None):\n",
    "        try:\n",
    "            self.data_dict = np.load('vgg19.npy', encoding='latin1').item()\n",
    "        except:\n",
    "            print(\"Place vgg19.npy in the same directory as of this notebook.\")\n",
    "            print(\"You can get it from \")\n",
    "        self.isTrain = True\n",
    "        self.epochs = 50\n",
    "        self.batch_size = 10\n",
    "        self.data_size = 400\n",
    "        self.restore = False\n",
    "        self.decay = 0.99\n",
    "        self.model_name = 'model'\n",
    "        self.ckpt_dir = './' + self.model_name + \"/checkpoint\"\n",
    "        self.test_fraction = 0.5\n",
    "        \n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        load variable from npy to build the VGG\n",
    "\n",
    "        :param rgb: rgb image [batch, height, width, 3] values scaled [0, 1]\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.variable_scope('Input') as scope:   \n",
    "            self.rgb = tf.placeholder(name = \"Input\", shape = [None, 224, 224, 3], dtype = tf.float32)\n",
    "            self.labels = tf.placeholder(name = \"Labels\", shape = [None, 4], dtype = tf.float32)\n",
    "            self.isTrain = tf.placeholder(name = \"isTrain\", shape = None, dtype = tf.bool)\n",
    "            self.step = tf.train.get_or_create_global_step()\n",
    "            self.lr = tf.placeholder(name = \"LR\", shape = None, dtype = tf.float32)\n",
    "\n",
    "        # Convert RGB to BGR\n",
    "        red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=self.rgb)\n",
    "        bgr = tf.concat(axis=3, values=[\n",
    "            blue - VGG_MEAN[0],\n",
    "            green - VGG_MEAN[1],\n",
    "            red - VGG_MEAN[2],\n",
    "        ])\n",
    "        \n",
    "        self.conv1_1 = self.conv_layer(bgr, \"conv1_1\")\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, \"conv1_2\")\n",
    "        self.pool1 = self.max_pool(self.conv1_2, 'pool1')\n",
    "\n",
    "        self.conv2_1 = self.conv_layer(self.pool1, \"conv2_1\")\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, \"conv2_2\")\n",
    "        self.pool2 = self.max_pool(self.conv2_2, 'pool2')\n",
    "\n",
    "        self.conv3_1 = self.conv_layer(self.pool2, \"conv3_1\")\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, \"conv3_2\")\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, \"conv3_3\")\n",
    "        self.conv3_4 = self.conv_layer(self.conv3_3, \"conv3_4\")\n",
    "        self.pool3 = self.max_pool(self.conv3_4, 'pool3')\n",
    "\n",
    "        self.conv4_1 = self.conv_layer(self.pool3, \"conv4_1\")\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, \"conv4_2\")\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, \"conv4_3\")\n",
    "        self.conv4_4 = self.conv_layer(self.conv4_3, \"conv4_4\")\n",
    "        self.pool4 = self.max_pool(self.conv4_4, 'pool4')\n",
    "\n",
    "        output = self.pool4\n",
    "\n",
    "        self.conv5_1 = self.conv_layer(self.pool4, \"conv5_1\")\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, \"conv5_2\")\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, \"conv5_3\")\n",
    "        self.conv5_4 = self.conv_layer(self.conv5_3, \"conv5_4\")\n",
    "        self.pool5 = self.max_pool(self.conv5_4, 'pool5')\n",
    "\n",
    "        self.fc6 = self.fc_layer(self.pool5, \"fc6\")\n",
    "        assert self.fc6.get_shape().as_list()[1:] == [4096]\n",
    "        self.relu6 = tf.nn.relu(self.fc6)\n",
    "\n",
    "        self.fc7 = self.fc_layer(self.relu6, \"fc7\")\n",
    "        self.relu7 = tf.nn.relu(self.fc7)\n",
    "\n",
    "        self.logits = self.fc_layer_output(self.relu7, \"fc8\")\n",
    "        #self.logits = self.fc_layer_output(self.relu6, \"fc7\")\n",
    "    \n",
    "        with tf.variable_scope(\"Loss\") as scope:\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.labels))\n",
    "\n",
    "        with tf.variable_scope(\"Accuracy\") as scope:\n",
    "\n",
    "            self.logits_max_args = tf.argmax(self.logits, axis = 1)\n",
    "            self.labels_max_args = tf.argmax(self.labels, axis = 1)\n",
    "            self.equal = tf.reduce_sum(tf.cast(tf.equal(self.logits_max_args, self.labels_max_args), tf.float32))\n",
    "            self.batch_acc = tf.divide(self.equal, tf.cast(tf.shape(self.logits)[0], tf.float32))\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\") as scope:\n",
    "\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss, self.step)\n",
    "            self.saver = tf.train.Saver(max_to_keep = 3)\n",
    "    \n",
    "    def avg_pool(self, bottom, name):\n",
    "        return tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def max_pool(self, bottom, name):\n",
    "        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def conv_layer(self, bottom, name):\n",
    "        with tf.variable_scope(name):\n",
    "            filt = self.get_conv_filter(name)\n",
    "\n",
    "            conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "            conv_biases = self.get_bias(name)\n",
    "            bias = tf.nn.bias_add(conv, conv_biases)\n",
    "\n",
    "            relu = tf.nn.relu(bias)\n",
    "            return relu\n",
    "\n",
    "    def fc_layer(self, bottom, name):\n",
    "        with tf.variable_scope(name):\n",
    "            shape = bottom.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(bottom, [-1, dim])\n",
    "\n",
    "            weights = self.get_fc_weight(name)\n",
    "            biases = self.get_bias(name)\n",
    "\n",
    "            # Fully connected layer. Note that the '+' operation automatically\n",
    "            # broadcasts the biases.\n",
    "            fc = tf.nn.bias_add(tf.matmul(x, weights), biases)\n",
    "\n",
    "            return fc\n",
    "\n",
    "    def fc_layer_output(self, x, name):\n",
    "        with tf.variable_scope(name):\n",
    "            shape = x.get_shape().as_list()\n",
    "            xavier = tf.contrib.layers.xavier_initializer()\n",
    "            weights = tf.get_variable(\"Weight\", shape=[shape[1], 4], initializer = xavier, dtype = tf.float32)\n",
    "            biases = tf.get_variable(\"Bias\", shape=[4], initializer = xavier, dtype = tf.float32)\n",
    "\n",
    "            # Fully connected layer. Note that the '+' operation automatically\n",
    "            # broadcasts the biases.\n",
    "            fc = tf.nn.bias_add(tf.matmul(x, weights), biases)\n",
    "\n",
    "            return fc\n",
    "        \n",
    "    def get_conv_filter(self, name):\n",
    "        initializer = self.data_dict[name][0]\n",
    "        return tf.get_variable(\"Filter\", initializer = initializer, dtype = tf.float32)\n",
    "\n",
    "    def get_bias(self, name):\n",
    "        initializer = self.data_dict[name][1]\n",
    "        return tf.get_variable(\"Bias\", initializer = initializer, dtype = tf.float32)\n",
    "\n",
    "    def get_fc_weight(self, name):\n",
    "        initializer = self.data_dict[name][0]\n",
    "        return tf.get_variable(\"Weight\", initializer = initializer, dtype = tf.float32)\n",
    "    \n",
    "    def train(self):\n",
    "        tf.reset_default_graph()\n",
    "        print('Loading Data')\n",
    "        train_data, train_labels, test_data, test_labels = load_data()\n",
    "        print('Data Loaded')\n",
    "        \n",
    "        print('Loading Model')\n",
    "        self.model()\n",
    "        print('Model Loaded')\n",
    "        \n",
    "        if not os.path.exists(self.model_name):\n",
    "            os.mkdir(self.model_name)\n",
    "        \n",
    "        test_data_size = int(self.data_size * self.test_fraction)\n",
    "        train_data_size = self.data_size - test_data_size\n",
    "\n",
    "        train_batches = train_data_size // self.batch_size\n",
    "        test_batches = test_data_size // self.batch_size\n",
    "\n",
    "        #data = ((data/255) * 2) - 1 # [0,255] --> [-1,1]\n",
    "        \n",
    "        with tf.Session() as self.sess:\n",
    "\n",
    "           \n",
    "            init_op = tf.global_variables_initializer()\n",
    "            self.sess.run(init_op)\n",
    "            parameter_count = tf.reduce_sum([tf.reduce_prod(tf.shape(v)) for v in tf.trainable_variables()])\n",
    "            \n",
    "            print('Parameters:', parameter_count.eval()) \n",
    "            start = 0\n",
    "            acc_best = -1\n",
    "            lr = 0.0001\n",
    "\n",
    "            data_perm = np.random.permutation(train_data_size)\n",
    "            train_perm = data_perm\n",
    "            test_perm = np.copy(data_perm)\n",
    "\n",
    "            print('Training commences from epoch ', start)\n",
    "\n",
    "            for i in range(start, self.epochs):\n",
    "                \n",
    "                count = 0\n",
    "                count_test = 0\n",
    "                avg_loss = 0\n",
    "\n",
    "                shuffle = np.random.permutation(train_data_size)\n",
    "                train_perm = train_perm[shuffle]\n",
    "\n",
    "                if i+1 == 50:\n",
    "                    lr = lr/10\n",
    "                \n",
    "                for j in range(train_batches):\n",
    "                    \n",
    "                    begin = time.time()\n",
    "                    \n",
    "                    if j != train_batches-1 :\n",
    "                        current_batch = train_perm[j*self.batch_size : (j+1)*self.batch_size]\n",
    "                        x = train_data[current_batch]\n",
    "                        y = train_labels[current_batch]\n",
    "                    else:\n",
    "                        current_batch = train_perm[j*self.batch_size: ]\n",
    "                        x = train_data[current_batch]\n",
    "                        y = train_labels[current_batch]\n",
    "\n",
    "                    feed_train = {self.rgb: x, self.labels: y, self.lr: lr}\n",
    "                    \n",
    "                    _, eq, loss = self.sess.run([self.optimizer, self.equal, self.loss], feed_dict = feed_train)\n",
    "                    count += eq\n",
    "                    avg_loss += loss\n",
    "                    \n",
    "                    line = 'Batch: %d Batch Accuracy: %.4f Loss: %.4f Time/Batch: %.4f' %(j, eq/len(current_batch), float(loss), time.time() - begin)\n",
    "                    print(line, end ='\\r')\n",
    "\n",
    "                for j in range(test_batches):\n",
    "                    \n",
    "                    print('======================Testing====================', end = '\\r')\n",
    "                    begin = time.time()\n",
    "                    \n",
    "                    if j != test_batches-1 : \n",
    "                        current_batch = test_perm[j*self.batch_size: (j+1)*self.batch_size]\n",
    "                        x = test_data[current_batch]\n",
    "                        y = test_labels[current_batch]\n",
    "                    else:\n",
    "                        current_batch = test_perm[j*self.batch_size: ]\n",
    "                        x = test_data[current_batch]\n",
    "                        y = test_labels[current_batch]\n",
    "\n",
    "                    feed_test = {self.rgb: x, self.labels: y}\n",
    "                    eq = self.sess.run(self.equal, feed_dict = feed_test)\n",
    "                    #print(eq)\n",
    "                    count_test += eq\n",
    "                \n",
    "                accuracy = count / train_data_size\n",
    "                accuracy_test = count_test / test_data_size\n",
    "                avg_loss = avg_loss / train_batches\n",
    "\n",
    "                line = \"Epoch: %d Train Acc: %.6f Test Acc: %.6f Average Loss/Batch: %.6f\" %(i,accuracy,accuracy_test,avg_loss)\n",
    "                print(line)\n",
    "\n",
    "                with open(os.path.join(self.model_name,'logs.txt'), 'a') as f :\n",
    "                    line += '\\n'\n",
    "                    f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Vgg19()\n",
    "net.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
